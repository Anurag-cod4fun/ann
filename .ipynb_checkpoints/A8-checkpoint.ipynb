{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e52f10e0-174c-4a6c-a30b-967804ca0db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 24.8822\n",
      "Epoch 100: Loss = 3.7989\n",
      "Epoch 200: Loss = 2.2884\n",
      "Epoch 300: Loss = 1.7302\n",
      "Epoch 400: Loss = 1.4352\n",
      "Epoch 500: Loss = 1.2384\n",
      "Epoch 600: Loss = 1.0907\n",
      "Epoch 700: Loss = 0.9718\n",
      "Epoch 800: Loss = 0.8742\n",
      "Epoch 900: Loss = 0.7949\n",
      "Test Accuracy: 0.6950\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, hidden_layers, hidden_neurons, output_neurons):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = []\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "    def initialize_parameters(self, input_shape):\n",
    "        np.random.seed(0)\n",
    "        self.weights.append(np.random.randn(self.hidden_neurons, input_shape))\n",
    "        self.biases.append(np.zeros((self.hidden_neurons, 1)))\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            self.weights.append(np.random.randn(self.hidden_neurons, self.hidden_neurons))\n",
    "            self.biases.append(np.zeros((self.hidden_neurons, 1)))\n",
    "        self.weights.append(np.random.randn(self.output_neurons, self.hidden_neurons))\n",
    "        self.biases.append(np.zeros((self.output_neurons, 1)))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = []\n",
    "        a = X\n",
    "\n",
    "        for i in range(self.hidden_layers):\n",
    "            z = np.dot(self.weights[i], a) + self.biases[i]\n",
    "            a = self.relu(z)\n",
    "            self.activations.append(a)\n",
    "\n",
    "        z = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        a = self.softmax(z)\n",
    "        self.activations.append(a)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward_propagation(self, X, y, learning_rate):\n",
    "        m = X.shape[1]\n",
    "        grad_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        delta = self.activations[-1] - y\n",
    "\n",
    "        for i in range(self.hidden_layers, 0, -1):\n",
    "            grad_weights[i] = np.dot(delta, self.activations[i-1].T) / m\n",
    "            grad_biases[i] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "            delta = np.dot(self.weights[i].T, delta) * (self.activations[i-1] > 0)\n",
    "\n",
    "        grad_weights[0] = np.dot(delta, X.T) / m\n",
    "        grad_biases[0] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * grad_weights[i]\n",
    "            self.biases[i] -= learning_rate * grad_biases[i]\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        input_shape = X.shape[0]\n",
    "        output_shape = y.shape[0]\n",
    "        self.initialize_parameters(input_shape)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward_propagation(X)\n",
    "            self.backward_propagation(X, y, learning_rate)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                loss = -np.mean(np.sum(y * np.log(output), axis=0))\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward_propagation(X)\n",
    "        return np.argmax(output, axis=0)\n",
    "\n",
    "# Example usage\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=4, n_clusters_per_class=1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_train_onehot = np.eye(len(encoder.classes_))[y_train_encoded].T\n",
    "\n",
    "# Create and train the neural network\n",
    "nn = NeuralNetwork(hidden_layers=1, hidden_neurons=100, output_neurons=len(encoder.classes_))\n",
    "nn.train(X_train.T, y_train_onehot, learning_rate=0.01, epochs=1000)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "y_pred = nn.predict(X_test.T)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04336916-97d9-47ef-9e88-8e541a2c9c45",
   "metadata": {},
   "source": [
    "# Explaination\n",
    "In this example, we use the synthetic data generated by make_classification from sklearn.datasets to perform multi-class classification. The neural network architecture consists of one hidden layer with 100 neurons and an output layer with the number of neurons equal to the number of classes in the data.\n",
    "\n",
    "The activation function used in the hidden layer is ReLU (Rectified Linear Unit), and the output layer uses the softmax activation function for multi-class classification.\n",
    "\n",
    "The neural network is trained using the provided training set and labels by performing forward propagation and backward propagation (gradient descent) with a specified learning rate and number of epochs. The network is then used to predict the labels for the test data, and the accuracy is calculated using accuracy_score from sklearn.metrics.\n",
    "\n",
    "Feel free to modify the code to suit your specific data and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfaa266-594e-481d-a709-eb0bf67b1d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
